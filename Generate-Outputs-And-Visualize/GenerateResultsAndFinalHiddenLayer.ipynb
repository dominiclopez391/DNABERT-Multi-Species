{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14030497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, DNATokenizer, BertConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea44cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from original DNABERT prediction module\n",
    "\n",
    "from run_finetune import MODEL_CLASSES, ALL_MODELS, processors\n",
    "from run_finetune import load_and_cache_examples\n",
    "from run_finetune import SequentialSampler, DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f421c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_test_csv = '/pathToTestCSV'\n",
    "path_to_model = '/pathToFinetunedModel'\n",
    "\n",
    "# modified multi-species classification task\n",
    "task_name = 'dna-genome-classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0649a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize multispecies DNABERT model with the same parameters as original DNABERT\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Required parameters\n",
    "parser.add_argument(\n",
    "    \"--data_dir\",\n",
    "    default=path_to_test_csv,\n",
    "    type=str,\n",
    "    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_type\",\n",
    "    default='dna',\n",
    "    type=str,\n",
    "    help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_process\",\n",
    "    default=2,\n",
    "    type=int,\n",
    "    help=\"number of processes used for data process\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--should_continue\", action=\"store_true\", help=\"Whether to continue from latest checkpoint in output_dir\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name_or_path\",\n",
    "    default=path_to_model,\n",
    "    type=str,\n",
    "    help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--task_name\",\n",
    "    default=task_name,\n",
    "    type=str,\n",
    "    help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_dir\",\n",
    "    default=\"output\",\n",
    "    type=str,\n",
    "    help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    ")\n",
    "\n",
    "\n",
    "# Other parameters\n",
    "parser.add_argument(\n",
    "    \"--visualize_data_dir\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--result_dir\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    help=\"The directory where the dna690 and mouse will save results.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--tokenizer_name\",\n",
    "    default=\"\",\n",
    "    type=str,\n",
    "    help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\",\n",
    "    default=\"\",\n",
    "    type=str,\n",
    "    help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--predict_dir\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    help=\"The output directory of predicted result. (when do_predict)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_seq_length\",\n",
    "    default=150,\n",
    "    type=int,\n",
    "    help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "    \"than this will be truncated, sequences shorter will be padded.\",\n",
    ")\n",
    "parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to do prediction on the given dataset.\")\n",
    "parser.add_argument(\"--do_visualize\", action=\"store_true\", help=\"Whether to calculate attention score.\")\n",
    "parser.add_argument(\"--visualize_train\", action=\"store_true\", help=\"Whether to visualize train.tsv or dev.tsv.\")\n",
    "parser.add_argument(\"--do_ensemble_pred\", action=\"store_true\", help=\"Whether to do ensemble prediction with kmer 3456.\")\n",
    "parser.add_argument(\n",
    "    \"--evaluate_during_training\", action=\"store_true\", help=\"Run evaluation during training at each logging step.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--per_gpu_pred_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for prediction.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--early_stop\", default=0, type=int, help=\"set this to a positive integet if you want to perfrom early stop. The model will stop \\\n",
    "                                                if the auc keep decreasing early_stop times\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--predict_scan_size\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    ")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--beta1\", default=0.9, type=float, help=\"Beta1 for Adam optimizer.\")\n",
    "parser.add_argument(\"--beta2\", default=0.999, type=float, help=\"Beta2 for Adam optimizer.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "parser.add_argument(\"--attention_probs_dropout_prob\", default=0.1, type=float, help=\"Dropout rate of attention.\")\n",
    "parser.add_argument(\"--hidden_dropout_prob\", default=0.1, type=float, help=\"Dropout rate of intermidiete layer.\")\n",
    "parser.add_argument(\"--rnn_dropout\", default=0.0, type=float, help=\"Dropout rate of intermidiete layer.\")\n",
    "parser.add_argument(\"--rnn\", default=\"lstm\", type=str, help=\"What kind of RNN to use\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=2, type=int, help=\"Number of rnn layers in dnalong model.\")\n",
    "parser.add_argument(\"--rnn_hidden\", default=768, type=int, help=\"Number of hidden unit in a rnn layer.\")\n",
    "parser.add_argument(\n",
    "    \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_steps\",\n",
    "    default=-1,\n",
    "    type=int,\n",
    "    help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
    ")\n",
    "parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "parser.add_argument(\"--warmup_percent\", default=0, type=float, help=\"Linear warmup over warmup_percent*total_steps.\")\n",
    "\n",
    "parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n",
    "parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n",
    "parser.add_argument(\n",
    "    \"--save_total_limit\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=\"Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_all_checkpoints\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
    ")\n",
    "parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "parser.add_argument(\n",
    "    \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_models\", type=int, default=None, help=\"The model used to do visualization. If None, use 3456.\",\n",
    ")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
    "\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--fp16\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--fp16_opt_level\",\n",
    "    type=str,\n",
    "    default=\"O1\",\n",
    "    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "    \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    ")\n",
    "parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f377ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\", 0)\n",
    "\n",
    "# Code for checking memory usage\n",
    "def device_mem():\n",
    "    free_memory, total_memory = torch.cuda.mem_get_info(device)\n",
    "    \n",
    "    print(f'mem allocated: {(total_memory - free_memory)/(1073741824)} GB')\n",
    "    print(f'mem free: {free_memory/(1073741824)} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "466dfa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "mem allocated: 6.6700439453125 GB\n",
      "mem free: 5.24249267578125 GB\n"
     ]
    }
   ],
   "source": [
    "# Initialize DNABERT model to output hidden states\n",
    "model = BertForSequenceClassification.from_pretrained('checkpoint-176000', output_hidden_states=True)\n",
    "tokenizer = DNATokenizer.from_pretrained('checkpoint-176000', do_lower_case=args.do_lower_case)\n",
    "\n",
    "device_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c7225fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "test_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85f087fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set manual seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# initialize sampler to sample randomly without replacement\n",
    "pred_sampler = RandomSampler(test_dataset, replacement=False)\n",
    "pred_dataloader = DataLoader(test_dataset, sampler=pred_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d4d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_input = tokenizer.encode([\"AAAAAA\", \"CGGGCC\", \"CGGGCC\", \"CGGGCC\"], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d40408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem allocated: 6.6700439453125 GB\n",
      "mem free: 5.24249267578125 GB\n",
      "mem allocated: 7.0567626953125 GB\n",
      "mem free: 4.85577392578125 GB\n"
     ]
    }
   ],
   "source": [
    "device_mem()\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "device_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d659f1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 2947/66666 [00:27<09:44, 108.92it/s]"
     ]
    }
   ],
   "source": [
    "# generate outputs and the final layers of the hidden states\n",
    "# store them in preds array\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "preds = []\n",
    "\n",
    "num_batches = 0\n",
    "\n",
    "for batch in tqdm(pred_dataloader):\n",
    "    num_batches += 1\n",
    "    if num_batches == 10000:\n",
    "        break\n",
    "    # device_mem()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=batch[0], attention_mask=batch[1])\n",
    "        ground_truth = batch[3].detach().cpu().numpy()\n",
    "        predictions = output[0].detach().cpu().numpy()\n",
    "        # hidden_state = output[1][-1].detach().cpu().numpy()\n",
    "        hidden_state = output[1][-1]\n",
    "        '''\n",
    "        for hidden_tensor in output[1]:\n",
    "            \n",
    "            \n",
    "            \n",
    "            if hidden_state is None:\n",
    "                hidden_state = hidden_tensor.detach().cpu().numpy()\n",
    "            else:\n",
    "                hidden_state = np.vstack((hidden_state, hidden_tensor.detach().cpu().numpy()))\n",
    "                \n",
    "        '''\n",
    "\n",
    "        preds.append([ground_truth, predictions, hidden_state])\n",
    "        #preds.append([ground_truth, predictions])\n",
    "    batch = tuple(t.to('cpu') for t in batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad9ffaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds[0][2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bdff687d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([6]),\n",
       " array([[-5.7757115 ,  0.50725484, -5.3489494 , -5.795606  , -3.8426554 ,\n",
       "         -5.2826967 ,  8.869636  , -1.574056  ,  2.6732917 , -1.971483  ,\n",
       "         -3.802625  , -4.2452264 , -2.2954338 , -1.6202209 , -4.0202074 ,\n",
       "         -4.16784   , -3.2110126 , -4.9556484 , -4.660567  , -2.9408958 ,\n",
       "         -2.1119242 , -3.3046687 , -2.8836193 , -4.597231  , -1.5807074 ,\n",
       "         -1.5889636 ]], dtype=float32),\n",
       " tensor([[[ 1.4950,  0.1428, -1.0611,  ...,  0.4944, -1.7593, -0.1664],\n",
       "          [ 0.7785,  0.4851, -0.7086,  ..., -0.6719, -0.7252,  0.5714],\n",
       "          [ 0.5982,  0.4157, -0.1231,  ..., -1.6310,  0.4971, -0.0163],\n",
       "          ...,\n",
       "          [ 2.0788, -0.0275, -1.8271,  ...,  1.4939, -0.5857, -0.4547],\n",
       "          [ 2.0787, -0.0275, -1.8271,  ...,  1.4940, -0.5856, -0.4547],\n",
       "          [ 2.0787, -0.0276, -1.8271,  ...,  1.4939, -0.5856, -0.4547]]],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# every element in the array contains: true label, probabilities for each species, and final layer of hidden state\n",
    "\n",
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3822f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save array in pt file\n",
    "\n",
    "torch.save(preds, \"outputs.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
